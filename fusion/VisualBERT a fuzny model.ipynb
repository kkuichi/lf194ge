{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9fWz-mJTmUR4","executionInfo":{"status":"ok","timestamp":1744829998794,"user_tz":-120,"elapsed":24814,"user":{"displayName":"lauri k","userId":"16133368723613273782"}},"outputId":"aa6ef6ac-a722-4dc0-ebc0-0c8032bf294b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","source":["import os\n","import json\n","import torch\n","import torch.nn as nn\n","import torchvision.models as models\n","import torchvision.transforms as transforms\n","from torch.utils.data import Dataset, DataLoader\n","from PIL import Image\n","import pickle\n","from transformers import VisualBertModel\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","from tqdm import tqdm\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","#inicializácia BiLSTM modelu\n","class BiLSTM_Model(nn.Module):\n","    def __init__(self):\n","        super(BiLSTM_Model, self).__init__()\n","        self.embedding = nn.Embedding(10000, 100)\n","        self.lstm1 = nn.LSTM(100, 128, batch_first=True, bidirectional=True)\n","        self.lstm2 = nn.LSTM(256, 64, batch_first=True, bidirectional=True)\n","        self.bn = nn.BatchNorm1d(128)\n","        self.fc = nn.Linear(128, 64)\n","\n","    def forward(self, x):\n","        x = self.embedding(x)\n","        x, _ = self.lstm1(x)\n","        x, _ = self.lstm2(x)\n","        x = x[:, -1, :]\n","        x = self.bn(x)\n","        return torch.relu(self.fc(x))\n","\n","#inicializácia ResNet extraktora\n","class ResNetFeatureExtractor(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        resnet = models.resnet50(pretrained=False)\n","        self.base = nn.Sequential(*list(resnet.children())[:-1])\n","        self.fc = nn.Linear(2048, 128)\n","\n","    def forward(self, x):\n","        x = self.base(x)\n","        x = x.view(x.size(0), -1)\n","        return self.fc(x)\n","\n","#trieda dataset\n","class FusionJSONLDataset(Dataset):\n","    def __init__(self, jsonl_path, img_root, vocab):\n","        self.data = [json.loads(l.strip()) for l in open(jsonl_path)]\n","        self.img_root = img_root\n","        self.vocab = vocab\n","        self.transform = transforms.Compose([\n","            transforms.Resize((224, 224)), transforms.ToTensor(),\n","            transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n","        ])\n","\n","    def text_to_seq(self, text, max_len=100):\n","        seq = [self.vocab.get(w, 1) for w in text.split()]\n","        return seq[:max_len] + [0]*(max_len - len(seq))\n","\n","    def __getitem__(self, idx):\n","        item = self.data[idx]\n","        img = Image.open(os.path.join(self.img_root, item[\"img\"])).convert(\"RGB\")\n","        text = torch.tensor(self.text_to_seq(item[\"text\"]), dtype=torch.long)\n","        label = torch.tensor(item[\"label\"], dtype=torch.float32)\n","        return self.transform(img), text, label\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","#trénovanie s VisualBERT\n","def train_fusion():\n","    jsonl = \"/content/drive/MyDrive/data/train.jsonl\"\n","    img_root = \"/content/drive/MyDrive\"\n","    vocab_path = \"/content/drive/MyDrive/tokenizer.pickle\"\n","    resnet_path = \"/content/drive/MyDrive/resnet_own.pth\"\n","    lstm_path = \"/content/drive/MyDrive/bilstm_model.pth\"\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    #načítanie tokenizera\n","    with open(vocab_path, \"rb\") as f:\n","        tokenizer = pickle.load(f)\n","    vocab = tokenizer if isinstance(tokenizer, dict) else tokenizer.word_index\n","\n","    dataset = FusionJSONLDataset(jsonl, img_root, vocab)\n","    loader = DataLoader(dataset, batch_size=16, shuffle=True)\n","\n","    #modely obrazok a text\n","    resnet = ResNetFeatureExtractor().to(device)\n","    state = torch.load(resnet_path, map_location=device)\n","    state.pop(\"resnet.fc.weight\", None)\n","    state.pop(\"resnet.fc.bias\", None)\n","    resnet.load_state_dict(state, strict=False)\n","    for p in resnet.parameters():\n","        p.requires_grad = False\n","    resnet.eval()\n","\n","    bilstm = BiLSTM_Model().to(device)\n","    bilstm.load_state_dict(torch.load(lstm_path, map_location=device), strict=False)\n","    for p in bilstm.parameters():\n","        p.requires_grad = False\n","    bilstm.eval()\n","\n","    visualbert = VisualBertModel.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\").to(device)\n","    projection = nn.Linear(192, 768).to(device)\n","\n","    #optimizer a stratová funkcia\n","    optimizer = torch.optim.Adam(\n","        list(projection.parameters()) + list(visualbert.parameters()), lr=2e-5\n","    )\n","    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([2.0]).to(device))\n","\n","    #trénovanie modelu VisualBERT\n","    num_epochs = 5\n","    for epoch in range(num_epochs):\n","        visualbert.train()\n","        running_loss = 0\n","\n","        for imgs, txts, labels in tqdm(loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n","            imgs, txts, labels = imgs.to(device), txts.to(device), labels.to(device).unsqueeze(1)\n","\n","            with torch.no_grad():\n","                img_feat = resnet(imgs)\n","                txt_feat = bilstm(txts)\n","\n","            fused = torch.cat([img_feat, txt_feat], dim=1)\n","            embeds = projection(fused).unsqueeze(1)\n","\n","            token_types = torch.zeros(embeds.shape[:2], dtype=torch.long).to(device)\n","\n","            out = visualbert(inputs_embeds=embeds, token_type_ids=token_types)\n","            logits = out.last_hidden_state[:, 0, :].mean(dim=1, keepdim=True)\n","\n","            loss = criterion(logits, labels)\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","\n","        print(f\" Epoch {epoch+1}, Loss: {running_loss / len(loader):.4f}\")\n","\n","    #uloženie modelu\n","    torch.save(visualbert.state_dict(), \"/content/drive/MyDrive/visualbert_finetuned.pth\")\n","    print(\"VisualBERT bol dotrénovaný a uložený.\")\n","\n","if __name__ == \"__main__\":\n","    train_fusion()\n"],"metadata":{"id":"uQYA_XmZaC3f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def test_fusion():\n","    jsonl = \"/content/drive/MyDrive/data/test_seen.jsonl\"\n","    img_root = \"/content/drive/MyDrive\"\n","    vocab_path = \"/content/drive/MyDrive/tokenizer.pickle\"\n","    resnet_path = \"/content/drive/MyDrive/resnet_own.pth\"\n","    lstm_path = \"/content/drive/MyDrive/bilstm_model.pth\"\n","    visualbert_path = \"/content/drive/MyDrive/visualbert_finetuned.pth\"\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    with open(vocab_path, \"rb\") as f:\n","        tokenizer = pickle.load(f)\n","    vocab = tokenizer if isinstance(tokenizer, dict) else tokenizer.word_index\n","\n","    dataset = FusionJSONLDataset(jsonl, img_root, vocab)\n","    loader = DataLoader(dataset, batch_size=16, shuffle=False)\n","\n","    resnet = ResNetFeatureExtractor().to(device)\n","    state = torch.load(resnet_path, map_location=device)\n","    state.pop(\"resnet.fc.weight\", None)\n","    state.pop(\"resnet.fc.bias\", None)\n","    resnet.load_state_dict(state, strict=False)\n","    resnet.eval()\n","\n","    bilstm = BiLSTM_Model().to(device)\n","    bilstm.load_state_dict(torch.load(lstm_path, map_location=device), strict=False)\n","    bilstm.eval()\n","\n","    visualbert = VisualBertModel.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\").to(device)\n","    visualbert.load_state_dict(torch.load(visualbert_path, map_location=device))\n","    visualbert.eval()\n","\n","    projection = nn.Linear(192, 768).to(device)\n","\n","    all_preds = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for imgs, txts, labels in tqdm(loader, desc=\"Testing\"):\n","            imgs, txts = imgs.to(device), txts.to(device)\n","            labels = labels.to(device).unsqueeze(1)\n","\n","            img_feat = resnet(imgs)\n","            txt_feat = bilstm(txts)\n","            fused = torch.cat([img_feat, txt_feat], dim=1)\n","            embeds = projection(fused).unsqueeze(1)\n","            token_types = torch.zeros(embeds.shape[:2], dtype=torch.long).to(device)\n","\n","            out = visualbert(inputs_embeds=embeds, token_type_ids=token_types)\n","            logits = out.last_hidden_state[:, 0, :].mean(dim=1, keepdim=True)\n","            probs = torch.sigmoid(logits)\n","            preds = (probs > 0.5).float()\n","\n","            all_preds.extend(preds.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    acc = accuracy_score(all_labels, all_preds)\n","    prec = precision_score(all_labels, all_preds)\n","    rec = recall_score(all_labels, all_preds)\n","    f1 = f1_score(all_labels, all_preds)\n","\n","    print(f\"Accuracy:  {acc:.4f}\")\n","    print(f\"Precision: {prec:.4f}\")\n","    print(f\"Recall:    {rec:.4f}\")\n","    print(f\"F1 Score:  {f1:.4f}\")"],"metadata":{"id":"KPUdrqpBnSyB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_fusion()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hflQpJsCnV6C","executionInfo":{"status":"ok","timestamp":1742844594333,"user_tz":-60,"elapsed":270080,"user":{"displayName":"lauri k","userId":"16133368723613273782"}},"outputId":"216cfc99-ff7e-46af-9aa8-e7ed6137c676"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Testing: 100%|██████████| 63/63 [04:23<00:00,  4.19s/it]"]},{"output_type":"stream","name":"stdout","text":["Accuracy:  0.4900\n","Precision: 0.4900\n","Recall:    1.0000\n","F1 Score:  0.6577\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["def test_fusion():\n","    jsonl = \"/content/drive/MyDrive/data/test_unseen.jsonl\"\n","    img_root = \"/content/drive/MyDrive\"\n","    vocab_path = \"/content/drive/MyDrive/tokenizer.pickle\"\n","    resnet_path = \"/content/drive/MyDrive/resnet_own.pth\"\n","    lstm_path = \"/content/drive/MyDrive/bilstm_model.pth\"\n","    visualbert_path = \"/content/drive/MyDrive/visualbert_finetuned.pth\"\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    with open(vocab_path, \"rb\") as f:\n","        tokenizer = pickle.load(f)\n","    vocab = tokenizer if isinstance(tokenizer, dict) else tokenizer.word_index\n","\n","    dataset = FusionJSONLDataset(jsonl, img_root, vocab)\n","    loader = DataLoader(dataset, batch_size=16, shuffle=False)\n","\n","    resnet = ResNetFeatureExtractor().to(device)\n","    state = torch.load(resnet_path, map_location=device)\n","    state.pop(\"resnet.fc.weight\", None)\n","    state.pop(\"resnet.fc.bias\", None)\n","    resnet.load_state_dict(state, strict=False)\n","    resnet.eval()\n","\n","    bilstm = BiLSTM_Model().to(device)\n","    bilstm.load_state_dict(torch.load(lstm_path, map_location=device), strict=False)\n","    bilstm.eval()\n","\n","    visualbert = VisualBertModel.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\").to(device)\n","    visualbert.load_state_dict(torch.load(visualbert_path, map_location=device))\n","    visualbert.eval()\n","\n","    projection = nn.Linear(192, 768).to(device)\n","\n","    all_preds = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for imgs, txts, labels in tqdm(loader, desc=\"Testing\"):\n","            imgs, txts = imgs.to(device), txts.to(device)\n","            labels = labels.to(device).unsqueeze(1)\n","\n","            img_feat = resnet(imgs)\n","            txt_feat = bilstm(txts)\n","            fused = torch.cat([img_feat, txt_feat], dim=1)\n","            embeds = projection(fused).unsqueeze(1)\n","            token_types = torch.zeros(embeds.shape[:2], dtype=torch.long).to(device)\n","\n","            out = visualbert(inputs_embeds=embeds, token_type_ids=token_types)\n","            logits = out.last_hidden_state[:, 0, :].mean(dim=1, keepdim=True)\n","            probs = torch.sigmoid(logits)\n","            preds = (probs > 0.5).float()\n","\n","            all_preds.extend(preds.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    acc = accuracy_score(all_labels, all_preds)\n","    prec = precision_score(all_labels, all_preds)\n","    rec = recall_score(all_labels, all_preds)\n","    f1 = f1_score(all_labels, all_preds)\n","\n","    print(f\"Accuracy:  {acc:.4f}\")\n","    print(f\"Precision: {prec:.4f}\")\n","    print(f\"Recall:    {rec:.4f}\")\n","    print(f\"F1 Score:  {f1:.4f}\")\n","\n","test_fusion()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Th0ECzMjpeHF","executionInfo":{"status":"ok","timestamp":1742845408279,"user_tz":-60,"elapsed":536261,"user":{"displayName":"lauri k","userId":"16133368723613273782"}},"outputId":"f6ca68a2-30d6-4bce-dd5b-a2d1349a6b63"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n","  warnings.warn(msg)\n","Testing: 100%|██████████| 125/125 [08:50<00:00,  4.24s/it]"]},{"output_type":"stream","name":"stdout","text":["Accuracy:  0.3750\n","Precision: 0.3750\n","Recall:    1.0000\n","F1 Score:  0.5455\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["Jednoduchý fúzny model"],"metadata":{"id":"uWcAx0sq_sZA"}},{"cell_type":"code","source":["#zamrznutie ResNet modelu\n","for param in resnet_model.parameters():\n","    param.requires_grad = False\n","\n","#zamrznutie LSTM modelu\n","for param in lstm_model.parameters():\n","    param.requires_grad = False\n","\n","#trénovanie iba na fúznej vrstve\n","for param in fusion_model.fc1.parameters():\n","    param.requires_grad = True\n","for param in fusion_model.fc2.parameters():\n","    param.requires_grad = True\n","\n","print(\"ResNet a LSTM sú zamrznuté. Trénujeme iba fúznu vrstvu.\")\n","\n","import torch.optim as optim\n","\n","#nastavenie optimalizátora\n","optimizer = optim.Adam([\n","    {\"params\": fusion_model.fc1.parameters()},\n","    {\"params\": fusion_model.fc2.parameters()}\n","], lr=0.0005)\n","\n","#stratova funkcia\n","criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([2.0]).to(\"cpu\"))\n","\n","#parametre tréningu\n","num_epochs = 5\n","batch_size = 32\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"G-6AtTv7vAS2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import DataLoader, Dataset\n","import pandas as pd\n","import os\n","from PIL import Image\n","import torchvision.transforms as transforms\n","\n","class FusionDataset(Dataset):\n","    def __init__(self, csv_path, img_folder, vocab, max_length=100, transform=None):\n","        self.data = pd.read_csv(csv_path)\n","        self.img_folder = img_folder\n","        self.vocab = vocab\n","        self.max_length = max_length\n","        self.transform = transform if transform else transforms.Compose([\n","            transforms.Resize((224, 224)),\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","        ])\n","\n","    def text_to_sequence(self, text):\n","        if not isinstance(text, str):\n","            text = \"\"\n","        sequence = [self.vocab.get(word, 1) for word in text.split()]\n","        return sequence[:self.max_length] + [0] * (self.max_length - len(sequence))\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        row = self.data.iloc[idx]\n","\n","        #načítanie obrázka\n","        img_path = os.path.join(self.img_folder, row['img'])\n","        if not os.path.exists(img_path):\n","            print(f\" Chýbajúci súbor: {img_path}\")\n","            img_path = os.path.join(self.img_folder, \"default.jpg\")\n","\n","        image = Image.open(img_path).convert(\"RGB\")\n","        image = self.transform(image)\n","\n","        #tokenizácia textu\n","        text_sequence = self.text_to_sequence(row['text'])\n","\n","        #štítky\n","        label = torch.tensor(row['label'], dtype=torch.float32)\n","\n","        return image, torch.tensor(text_sequence, dtype=torch.long), label\n","\n","#načítanie datasetu\n","csv_path = \"/content/drive/MyDrive/data/new_merged.csv\"\n","img_folder = \"/content/drive/MyDrive/\"\n","vocab = {}\n","\n","dataset = FusionDataset(csv_path, img_folder, vocab)\n","train_size = int(0.8 * len(dataset))\n","val_size = len(dataset) - train_size\n","train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","\n","print(f\"Dataset načítaný: {len(train_dataset)} tréningových, {len(val_dataset)} validačných vzoriek\")"],"metadata":{"id":"ON_Y79eduvDR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torchvision.models as models\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from tqdm import tqdm\n","from collections import Counter\n","import pandas as pd\n","import os\n","from PIL import Image\n","\n","\n","import torch\n","import torch.nn as nn\n","import torchvision.models as models\n","\n","#BiLSTM model s výstupnou črtou (return_feature=True)\n","class BiLSTM_Model(nn.Module):\n","    def __init__(self, vocab_size=10000, embedding_dim=100, max_length=100, lstm_units=64, dropout_rate=0.5):\n","        super(BiLSTM_Model, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.lstm1 = nn.LSTM(embedding_dim, 128, batch_first=True, bidirectional=True)\n","        self.lstm2 = nn.LSTM(256, lstm_units, batch_first=True, bidirectional=True)\n","        self.batch_norm = nn.BatchNorm1d(2 * lstm_units)\n","        self.fc1 = nn.Linear(2 * lstm_units, 64)\n","        self.dropout = nn.Dropout(dropout_rate)\n","        self.fc2 = nn.Linear(64, 1)\n","\n","    def forward(self, x, return_feature=False):\n","        x = self.embedding(x)\n","        x, _ = self.lstm1(x)\n","        x, _ = self.lstm2(x)\n","        x = x[:, -1, :]\n","        x = self.batch_norm(x)\n","        feat = torch.relu(self.fc1(x))\n","        x = self.dropout(feat)\n","        out = self.fc2(x)\n","        return feat if return_feature else out\n","\n","#ResNet model bez FC vrstvy\n","class ResNetFeatureExtractor(nn.Module):\n","    def __init__(self):\n","        super(ResNetFeatureExtractor, self).__init__()\n","        resnet = models.resnet50(pretrained=False)\n","        self.resnet_features = nn.Sequential(*list(resnet.children())[:-1])\n","        self.fc = nn.Linear(2048, 128)\n","\n","    def forward(self, x):\n","        x = self.resnet_features(x)\n","        x = torch.flatten(x, 1)\n","        x = self.fc(x)\n","        return x\n","\n","\n","from torch.utils.data import Dataset\n","import pandas as pd\n","import os\n","from PIL import Image\n","import torch\n","import torchvision.transforms as transforms\n","\n","#opravený dataset, ktorý vracia (image, text, label)\n","from torch.utils.data import Dataset\n","import os\n","from PIL import Image\n","import torch\n","import torchvision.transforms as transforms\n","\n","class FusionDataset(Dataset):\n","    def __init__(self, csv_path, img_folder, vocab, max_length=100, transform=None):\n","        self.data = pd.read_csv(csv_path)\n","        self.img_folder = img_folder\n","        self.vocab = vocab\n","        self.max_length = max_length\n","        self.transform = transform if transform else transforms.Compose([\n","            transforms.Resize((224, 224)),\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","        ])\n","\n","    def text_to_sequence(self, text):\n","        if not isinstance(text, str):\n","            text = \"\"\n","        sequence = [self.vocab.get(word, 1) for word in text.split()]\n","        return sequence[:self.max_length] + [0] * (self.max_length - len(sequence))\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        row = self.data.iloc[idx]\n","\n","        #načítanie obrázka\n","        img_path = os.path.join(self.img_folder, row['img'])\n","        if not os.path.exists(img_path):\n","            print(f\" Chýbajúci súbor: {img_path}\")\n","            img_path = os.path.join(self.img_folder, \"default.jpg\")\n","\n","        image = Image.open(img_path).convert(\"RGB\")\n","        image = self.transform(image)\n","\n","        #tokenizácia textu\n","        text_sequence = self.text_to_sequence(row['text'])\n","\n","        #štítky\n","        label = torch.tensor(row['label'], dtype=torch.float32)\n","\n","        return image, torch.tensor(text_sequence, dtype=torch.long), label\n","\n","\n","\n","#inicializácia modelov\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","#načítanie ResNet modelu\n","resnet_model_path = \"/content/drive/MyDrive/resnet_own.pth\"\n","resnet_model = ResNetFeatureExtractor()\n","state_dict = torch.load(resnet_model_path, map_location=device)\n","del state_dict['resnet.fc.weight']\n","del state_dict['resnet.fc.bias']\n","resnet_model.load_state_dict(state_dict, strict=False)\n","resnet_model.eval()\n","\n","#načítanie CNN-LSTM modelu\n","lstm_model_path = \"/content/drive/MyDrive/bilstm_model.pth\"\n","lstm_model = BiLSTM_Model(vocab_size=10000, embedding_dim=100, max_length=100)\n","lstm_model.load_state_dict(torch.load(lstm_model_path, map_location=device))\n","lstm_model.eval()\n","\n","#zamrznutie parametrov\n","for param in resnet_model.parameters():\n","    param.requires_grad = False\n","for param in lstm_model.parameters():\n","    param.requires_grad = False\n","\n","#definícia fúzneho modelu (vážený priemer)\n","class ResNetLSTM_Fusion(nn.Module):\n","    def __init__(self, resnet_model, lstm_model):\n","        super(ResNetLSTM_Fusion, self).__init__()\n","        self.resnet = resnet_model\n","        self.lstm = lstm_model\n","\n","        #trénovateľné logit váhy pre fúziu\n","        self.logits = nn.Parameter(torch.tensor([0.0, 0.0]))\n","\n","        self.text_proj = nn.Linear(64, 128)\n","        self.batch_norm = nn.BatchNorm1d(128)\n","        self.fc1 = nn.Linear(128, 64)\n","        self.fc2 = nn.Linear(64, 1)\n","\n","    def forward(self, img_input, text_input):\n","        with torch.no_grad():\n","            img_feat = self.resnet(img_input)\n","            text_feat = self.lstm(text_input, return_feature=True)\n","\n","        text_feat = self.text_proj(text_feat)\n","\n","        #softmax z logitov, čiže normované váhy (0, 1), alpha + beta = 1\n","        weights = torch.softmax(self.logits, dim=0)\n","        alpha, beta = weights[0], weights[1]\n","\n","        fusion_feat = alpha * img_feat + beta * text_feat\n","        fusion_feat = self.batch_norm(fusion_feat)\n","        fusion_feat = torch.relu(self.fc1(fusion_feat))\n","        output = self.fc2(fusion_feat)\n","        return output\n","\n","#inicializácia fúzneho modelu\n","fusion_model = ResNetLSTM_Fusion(resnet_model, lstm_model).to(device)\n","optimizer = optim.Adam(filter(lambda p: p.requires_grad, fusion_model.parameters()), lr=0.0005)\n","criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([2.0]).to(device))\n","\n","#trénovanie iba fúznej vrstvy\n","num_epochs = 10\n","for epoch in range(num_epochs):\n","    fusion_model.train()\n","    running_loss = 0.0\n","    correct_train, total_train = 0, 0\n","\n","    for images, text_inputs, labels in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n","        images, text_inputs, labels = images.to(device), text_inputs.to(device), labels.to(device).float().unsqueeze(1)\n","        optimizer.zero_grad()\n","        outputs = fusion_model(images, text_inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","\n","    print(f\" Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n","\n","#uloženie modelu\n","torch.save(fusion_model.state_dict(), \"/content/drive/MyDrive/fusion_model.pth\")\n","print(\" Fúzny model bol uložený.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pp14MwNJuHfQ","executionInfo":{"status":"ok","timestamp":1742848809678,"user_tz":-60,"elapsed":2188822,"user":{"displayName":"lauri k","userId":"16133368723613273782"}},"outputId":"a5557b64-4c61-4604-ceac-1f773defd5dc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n","  warnings.warn(msg)\n","Training Epoch 1/10: 100%|██████████| 304/304 [05:42<00:00,  1.13s/it]\n"]},{"output_type":"stream","name":"stdout","text":[" Epoch [1/10], Loss: 0.9468\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 2/10: 100%|██████████| 304/304 [03:25<00:00,  1.48it/s]\n"]},{"output_type":"stream","name":"stdout","text":[" Epoch [2/10], Loss: 0.9415\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 3/10: 100%|██████████| 304/304 [03:21<00:00,  1.51it/s]\n"]},{"output_type":"stream","name":"stdout","text":[" Epoch [3/10], Loss: 0.9422\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 4/10: 100%|██████████| 304/304 [03:21<00:00,  1.51it/s]\n"]},{"output_type":"stream","name":"stdout","text":[" Epoch [4/10], Loss: 0.9410\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 5/10: 100%|██████████| 304/304 [03:23<00:00,  1.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":[" Epoch [5/10], Loss: 0.9402\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 6/10: 100%|██████████| 304/304 [03:27<00:00,  1.46it/s]\n"]},{"output_type":"stream","name":"stdout","text":[" Epoch [6/10], Loss: 0.9393\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 7/10: 100%|██████████| 304/304 [03:29<00:00,  1.45it/s]\n"]},{"output_type":"stream","name":"stdout","text":[" Epoch [7/10], Loss: 0.9387\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 8/10: 100%|██████████| 304/304 [03:25<00:00,  1.48it/s]\n"]},{"output_type":"stream","name":"stdout","text":[" Epoch [8/10], Loss: 0.9399\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 9/10: 100%|██████████| 304/304 [03:24<00:00,  1.48it/s]\n"]},{"output_type":"stream","name":"stdout","text":[" Epoch [9/10], Loss: 0.9380\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 10/10: 100%|██████████| 304/304 [03:25<00:00,  1.48it/s]\n"]},{"output_type":"stream","name":"stdout","text":[" Epoch [10/10], Loss: 0.9373\n"," Fúzny model bol uložený.\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","import numpy as np\n","\n","def test_fusion_model(fusion_model, test_loader, device):\n","    fusion_model.eval()\n","    all_preds = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for images, text_inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n","            images = images.to(device)\n","            text_inputs = text_inputs.to(device)\n","            labels = labels.to(device).unsqueeze(1)\n","\n","            outputs = fusion_model(images, text_inputs)\n","            probs = torch.sigmoid(outputs)\n","            preds = (probs > 0.5).float()\n","\n","            all_preds.extend(preds.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    # Výpočty metrík\n","    acc = accuracy_score(all_labels, all_preds)\n","    prec = precision_score(all_labels, all_preds)\n","    rec = recall_score(all_labels, all_preds)\n","    f1 = f1_score(all_labels, all_preds)\n","\n","    print(f\"\\n--- Evaluation Metrics ---\")\n","    print(f\"Accuracy : {acc:.4f}\")\n","    print(f\"Precision: {prec:.4f}\")\n","    print(f\"Recall   : {rec:.4f}\")\n","    print(f\"F1 Score : {f1:.4f}\")\n","\n","    # Zobrazenie naučených váh alpha a beta\n","    weights = torch.softmax(fusion_model.logits, dim=0)\n","    print(f\"\\nLearned weights -> alpha (image): {weights[0].item():.4f}, beta (text): {weights[1].item():.4f}\")"],"metadata":{"id":"exCEGxcm55Qf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class FusionJSONLDataset(Dataset):\n","    def __init__(self, jsonl_path, img_root, vocab, max_length=100, transform=None):\n","        self.data = [json.loads(line.strip()) for line in open(jsonl_path)]\n","        self.img_root = img_root\n","        self.vocab = vocab\n","        self.max_length = max_length\n","        self.transform = transform if transform else transforms.Compose([\n","            transforms.Resize((224, 224)),\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                 std=[0.229, 0.224, 0.225])\n","        ])\n","\n","    def text_to_sequence(self, text):\n","        if not isinstance(text, str):\n","            text = \"\"\n","        sequence = [self.vocab.get(word, 1) for word in text.split()]\n","        return sequence[:self.max_length] + [0] * (self.max_length - len(sequence))\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        item = self.data[idx]\n","        img_path = os.path.join(self.img_root, item[\"img\"])\n","        if not os.path.exists(img_path):\n","            print(f\"Missing image: {img_path}\")\n","            img_path = os.path.join(self.img_root, \"default.jpg\")\n","\n","        image = Image.open(img_path).convert(\"RGB\")\n","        image = self.transform(image)\n","\n","        text_sequence = self.text_to_sequence(item[\"text\"])\n","        label = torch.tensor(item[\"label\"], dtype=torch.float32)\n","\n","        return image, torch.tensor(text_sequence, dtype=torch.long), label"],"metadata":{"id":"kA2zL3MQ5c1E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#načítanie vocab\n","import pickle\n","with open(\"/content/drive/MyDrive/tokenizer.pickle\", \"rb\") as f:\n","    tokenizer = pickle.load(f)\n","vocab = tokenizer if isinstance(tokenizer, dict) else tokenizer.word_index\n","\n","#dataloader\n","test_jsonl = \"/content/drive/MyDrive/data/test_unseen.jsonl\"\n","img_root = \"/content/drive/MyDrive\"\n","\n","test_dataset = FusionJSONLDataset(test_jsonl, img_root, vocab)\n","test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"],"metadata":{"id":"x0ymqJr25c3l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#načítanie modelu\n","fusion_model = ResNetLSTM_Fusion(resnet_model, lstm_model).to(device)\n","fusion_model.load_state_dict(torch.load(\"/content/drive/MyDrive/fusion_model.pth\", map_location=device))\n","\n","#spustenie testovania\n","test_fusion_model(fusion_model, test_loader, device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EtNVWPnn6XAI","executionInfo":{"status":"ok","timestamp":1742849316541,"user_tz":-60,"elapsed":40765,"user":{"displayName":"lauri k","userId":"16133368723613273782"}},"outputId":"45b2eb81-17ec-4519-bc7e-4ffa4e43b777"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Testing: 100%|██████████| 125/125 [00:40<00:00,  3.09it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","--- Evaluation Metrics ---\n","Accuracy : 0.4980\n","Precision: 0.4072\n","Recall   : 0.7427\n","F1 Score : 0.5260\n","\n","Learned weights -> alpha (image): 0.5500, beta (text): 0.4500\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["#načítanie modelu\n","fusion_model = ResNetLSTM_Fusion(resnet_model, lstm_model).to(device)\n","fusion_model.load_state_dict(torch.load(\"/content/drive/MyDrive/fusion_model.pth\", map_location=device))\n","\n","#spustenie testovania\n","test_fusion_model(fusion_model, test_loader, device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y-ruW0X65c6Y","executionInfo":{"status":"ok","timestamp":1742849246884,"user_tz":-60,"elapsed":23865,"user":{"displayName":"lauri k","userId":"16133368723613273782"}},"outputId":"c650f563-ceaa-4780-e695-8f678184ab3a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Testing: 100%|██████████| 63/63 [00:23<00:00,  2.68it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","--- Evaluation Metrics ---\n","Accuracy : 0.5140\n","Precision: 0.5027\n","Recall   : 0.7551\n","F1 Score : 0.6036\n","\n","Learned weights -> alpha (image): 0.5500, beta (text): 0.4500\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["\n","print(fusion_model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_cXFynrQ5c93","executionInfo":{"status":"ok","timestamp":1742849702261,"user_tz":-60,"elapsed":18,"user":{"displayName":"lauri k","userId":"16133368723613273782"}},"outputId":"8fbe7f69-5ecf-4d4b-c5a0-9bd1f24406f5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ResNetLSTM_Fusion(\n","  (resnet): ResNetFeatureExtractor(\n","    (resnet_features): Sequential(\n","      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU(inplace=True)\n","      (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","      (4): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (5): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (3): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (6): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (3): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (4): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (5): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (7): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (8): AdaptiveAvgPool2d(output_size=(1, 1))\n","    )\n","    (fc): Linear(in_features=2048, out_features=128, bias=True)\n","  )\n","  (lstm): BiLSTM_Model(\n","    (embedding): Embedding(10000, 100)\n","    (lstm1): LSTM(100, 128, batch_first=True, bidirectional=True)\n","    (lstm2): LSTM(256, 64, batch_first=True, bidirectional=True)\n","    (batch_norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (fc1): Linear(in_features=128, out_features=64, bias=True)\n","    (dropout): Dropout(p=0.5, inplace=False)\n","    (fc2): Linear(in_features=64, out_features=1, bias=True)\n","  )\n","  (text_proj): Linear(in_features=64, out_features=128, bias=True)\n","  (batch_norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (fc1): Linear(in_features=128, out_features=64, bias=True)\n","  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",")\n"]}]},{"cell_type":"code","source":["!pip install torchsummary"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WgJxenLK8QKM","executionInfo":{"status":"ok","timestamp":1742849777563,"user_tz":-60,"elapsed":4678,"user":{"displayName":"lauri k","userId":"16133368723613273782"}},"outputId":"473ae413-ecd5-4425-9c5b-81c8adb8f7f4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torchsummary in /usr/local/lib/python3.11/dist-packages (1.5.1)\n"]}]}],"metadata":{"language_info":{"name":"python"},"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}